---
title: verl_multinode
top: false
cover: false
toc: true
mathjax: true
summary: åˆ†å¸ƒå¼ç¯å¢ƒä¸­ä½¿ç”¨ verl è¿›è¡Œå¤šèŠ‚ç‚¹è®­ç»ƒã€‚ä»å¯åŠ¨ Ray é›†ç¾¤åˆ°æäº¤è®­ç»ƒä»»åŠ¡ï¼Œå†åˆ°æ¨¡å‹åˆå¹¶å’Œè§£å†³å¸¸è§é—®é¢˜ã€‚
tags:
  - verl
  - multinode
categories: RLHF
abbrlink: 42939
date: 2025-06-11 16:34:52
password:
---

# verl multi-node train

## å¯åŠ¨ head èŠ‚ç‚¹

> ### âš ï¸ NumExprçš„çº¿ç¨‹æ•°è­¦å‘Š
>
> ```
> Note: detected 112 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
> Note: NumExpr detected 112 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
> NumExpr defaulting to 8 threads.
> ```
>
> ğŸ” **åŸå› **ï¼š
>
> - æœåŠ¡å™¨æ£€æµ‹åˆ° **112 ä¸ªè™šæ‹Ÿæ ¸å¿ƒ**ï¼ˆå¤šçº¿ç¨‹å¤„ç†å™¨ï¼‰ã€‚
> - `NumExpr`æ˜¯ä¸€ä¸ªé«˜æ•ˆçš„æ•°å€¼è®¡ç®—åº“ï¼ˆnumpy èƒŒåçš„è®¡ç®—åŠ é€Ÿï¼‰ï¼Œå®ƒé»˜è®¤å¯¹çº¿ç¨‹æ•°åšäº†å®‰å…¨é™åˆ¶ä»¥é˜²æ­¢è¿‡åº¦å ç”¨ CPU èµ„æºã€‚
> - å¦‚æœæ²¡æœ‰è®¾ç½® `NUMEXPR_MAX_THREADS` ç¯å¢ƒå˜é‡ï¼Œå®ƒä¼šå¼ºåˆ¶é™åˆ¶ä¸º **8 ä¸ªçº¿ç¨‹**ã€‚
>
> âœ… **è§£å†³åŠæ³•ï¼ˆå¯é€‰ï¼‰**ï¼š
>  å¦‚æœä½ éœ€è¦è®©NumExprä½¿ç”¨æ›´å¤šçº¿ç¨‹ï¼ˆæ¯”å¦‚æ¥è¿‘æ ¸å¿ƒæ•°ï¼‰ï¼Œå¯ä»¥è®¾ç½®ï¼š
>
> ```bash
> export NUMEXPR_MAX_THREADS=112
> ```
>
> å½“ç„¶ï¼Œè¿™é€šå¸¸åªæ˜¯æé†’ï¼Œå¹¶ä¸ä¼šå½±å“ Ray æˆ– Verl çš„è¿è¡Œï¼Œé™¤éä½ ç”¨åˆ°å¤§é‡NumExprè®¡ç®—ã€‚

åœ¨ head èŠ‚ç‚¹ä¸Šæ‰§è¡Œä»¥ä¸‹æŒ‡ä»¤ï¼Œå¯åŠ¨ ray 

```
ray start --head --dashboard-host=0.0.0.0
```

<img src="verl_multinode/image-20250529135709506.png" alt="Fig.1 start head node" style="zoom: 80%;" />



## å¯åŠ¨ worker èŠ‚ç‚¹

åœ¨ worker èŠ‚ç‚¹ä¸Šæ‰§è¡Œä»¥ä¸‹æŒ‡ä»¤ï¼ŒåŠ å…¥é›†ç¾¤

```bash
ray start --address='172.31.0.12:6379'
```

<img src="verl_multinode/image-20250529135907803.png" alt="Fig.2 add worker to cluster" style="zoom: 80%;" />



## æŸ¥çœ‹é›†ç¾¤ä¿¡æ¯

åœ¨ä»»æ„èŠ‚ç‚¹ä¸Šæ‰§è¡Œ ray statusï¼Œå¯ä»¥çœ‹åˆ°é›†ç¾¤çš„èŠ‚ç‚¹ä¿¡æ¯å’Œèµ„æºä¿¡æ¯ã€‚

<img src="verl_multinode/image-20250529140425815.png" alt="Fig.3 ray status" style="zoom: 80%;" />

ä¹Ÿå¯ä»¥ä½¿ç”¨ dashboard æŸ¥çœ‹é›†ç¾¤ä¿¡æ¯ã€‚

<img src="verl_multinode/image-20250529144350183.png" alt="Fig.4 ray dashboard" style="zoom: 80%;" />



## æ‰§è¡Œä»»åŠ¡

åªéœ€å°†éœ€è¦æ‰§è¡Œçš„ä»»åŠ¡åœ¨ä»»æ„ä¸€å°æœºå™¨ä¸Šå‘é›†ç¾¤æäº¤å³å¯ã€‚

1. åœ¨é…ç½®æ–‡ä»¶ä¸­ï¼ˆverl/trainer/runtime_env.yamlï¼‰åŠ ä¸Š GLOO_SOCKET_IFNAME ç¯å¢ƒå˜é‡ 

```
working_dir: ./
excludes: ["/.git/"]
env_vars:
  TORCH_NCCL_AVOID_RECORD_STREAMS: "1"
  # If you are using vllm<=0.6.3, you might need to set the following environment variable to avoid bugs:
  # VLLM_ATTENTION_BACKEND: "XFORMERS"
  GLOO_SOCKET_IFNAME: "eth0"
```



2. ç¼–è¾‘è®­ç»ƒçš„ shell è„šæœ¬å¹¶è¿è¡Œ

   æ³¨æ„ä¸‹é¢çš„è„šæœ¬æ˜¯åœ¨ head èŠ‚ç‚¹ä¸Šæ‰§è¡Œçš„ï¼Œè„šæœ¬ä¸­çš„ job submit åœ°å€å¡«çš„æ˜¯ 127.0.0.1ã€‚

```bash
set -x
ray job submit --address="http://127.0.0.1:8265" \
    --runtime-env=verl/trainer/runtime_env.yaml \
    --no-wait \
    -- \
python3 -m verl.trainer.main_ppo \
    algorithm.adv_estimator=grpo \
    data.train_files=/root/verl/data/hdfs/train.parquet \
    data.val_files=/root/verl/data/hdfs/test.parquet \
    data.train_batch_size=1024 \
    data.max_prompt_length=512 \
    data.max_response_length=1024 \
    data.filter_overlong_prompts=True \
    data.truncation='error' \
    actor_rollout_ref.model.path=Qwen/Qwen3-32B \
    actor_rollout_ref.actor.optim.lr=1e-6 \
    actor_rollout_ref.model.use_remove_padding=True \
    actor_rollout_ref.actor.ppo_mini_batch_size=128 \
    actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=16 \
    actor_rollout_ref.actor.use_kl_loss=True \
    actor_rollout_ref.actor.kl_loss_coef=0.001 \
    actor_rollout_ref.actor.kl_loss_type=low_var_kl \
    actor_rollout_ref.actor.entropy_coeff=0 \
    actor_rollout_ref.model.enable_gradient_checkpointing=True \
    actor_rollout_ref.actor.fsdp_config.param_offload=False \
    actor_rollout_ref.actor.fsdp_config.optimizer_offload=False \
    actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=16 \
    actor_rollout_ref.rollout.tensor_model_parallel_size=4 \
    actor_rollout_ref.rollout.name=vllm \
    actor_rollout_ref.rollout.gpu_memory_utilization=0.6 \
    actor_rollout_ref.rollout.n=8 \
    actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=8 \
    actor_rollout_ref.ref.fsdp_config.param_offload=True \
    algorithm.use_kl_in_reward=False \
    trainer.critic_warmup=0 \
    trainer.logger=['console','wandb'] \
    trainer.project_name='verl_grpo_example_gsm8k' \
    trainer.experiment_name='qwen3_32b_2nodes' \
    trainer.n_gpus_per_node=8 \  # æ¯ä¸ªèŠ‚ç‚¹ 8 GPU
    trainer.nnodes=2 \  # ä¸¤ä¸ªèŠ‚ç‚¹
    trainer.save_freq=20 \
    trainer.test_freq=5 \
    trainer.total_epochs=1 $@
```



## æµå¼æŸ¥çœ‹ log

ä½¿ç”¨å·²ä¸‹æŒ‡ä»¤åœ¨ç»ˆç«¯æŸ¥çœ‹æµå¼æ—¥å¿—ï¼Œæˆ–åœ¨ dashboard ç½‘é¡µä¸­æŸ¥çœ‹

```
ray job logs raysubmit_iCE8j5WD3fN5W9uX -f 
```



## æ¨¡å‹åˆå¹¶

ä¸¤ä¸ªèŠ‚ç‚¹åˆ†å¸ƒå¼è®­ç»ƒä¹‹åçš„æ¨¡å‹æƒé‡æ–‡ä»¶ä¼šåˆ†åˆ«ä¿å­˜åœ¨ä¸¤ä¸ªèŠ‚ç‚¹ä¸Šï¼Œæ­¤æ—¶æˆ‘ä»¬éœ€è¦å°†è®­ç»ƒå®Œçš„**æƒé‡åˆå¹¶åˆ°åŒä¸€ä¸ªèŠ‚ç‚¹çš„åŒä¸€ä¸ªç›®å½•ä¸­**ã€‚

å®˜æ–¹åœ¨ verl/scripts/model_merger.py ä¸­å®ç°äº†æ¨¡å‹åˆå¹¶è„šæœ¬ï¼Œç”¨äºå°† FSDP å’Œ Megatron åç«¯çš„ checkpoints åˆå¹¶æˆ huggingface æ¨¡å‹ã€‚

> - To merge FSDP checkpoints:
>
> ```sh
> python scripts/model_merger.py merge \
>     --backend fsdp \
>     --local_dir checkpoints/verl_fsdp_gsm8k_examples/qwen2_5_0b5_fsdp_saveload/global_step_1/actor \
>     --target_dir /path/to/merged_hf_model
> ```
>
> - To merge Megatron checkpoints:
>
> ```sh
> python scripts/model_merger.py merge \
>     --backend megatron \
>     --tie-word-embedding \
>     --local_dir checkpoints/verl_megatron_gsm8k_examples/qwen2_5_0b5_megatron_saveload/global_step_1/actor \
>     --target_dir /path/to/merged_hf_model
> ```

è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ FSDP çš„æ–¹å¼è®­ç»ƒçš„æ¨¡å‹ï¼Œæ‰€ä»¥æˆ‘ä»¬åœ¨ scripts/ è·¯å¾„ä¸‹ï¼Œæ–°å»ºè„šæœ¬ï¼Œç¼–è¾‘å¦‚ä¸‹ä¿¡æ¯ï¼š

```bash
python scripts/model_merger.py merge \
    --backend fsdp \
    --local_dir /tmp/ray/session_2025-05-28_15-53-09_958478_132003/runtime_resources/working_dir_files/_ray_pkg_c461f3378ff510ed/checkpoints/verl_grpo_example_gsm8k/qwen3_32b_2nodes/global_step_7/actor \
    --target_dir /data/models/verl_results
```

æ‰§è¡Œ shell è„šæœ¬ï¼Œå°±å¯ä»¥åœ¨æˆ‘ä»¬æŒ‡å®šçš„è·¯å¾„ä¸‹ç”Ÿæˆ huggingface çš„æ¨¡å‹ï¼Œä»¥ä¾¿åé¢çš„éƒ¨ç½²æ¨ç†ã€‚

<img src="verl_multinode/image-20250529141312378.png" alt="Fig.5 merge model" style="zoom: 80%;" />



## å‘

ValueError: Model architectures ['Qwen3ForCausalLM'] failed to be inspected. Please check the logs for more details.

```
ValueError: Model architectures ['Qwen3ForCausalLM'] failed to be inspected. Please check the logs for more details.

ERROR 11-29 06:51:24 registry.py:297] RuntimeError: Error raised in subprocess:
ERROR 11-29 06:51:24 registry.py:297] Error: mkl-service + Intel(R) MKL: MKL_THREADING_LAYER=INTEL is incompatible with libgomp-a34b3233.so.1 library.
ERROR 11-29 06:51:24 registry.py:297]   Try to import numpy first or set the threading layer accordingly. Set MKL_SERVICE_FORCE_INTEL to force it.
```

[github issue](https://github.com/vllm-project/vllm/issues/10759) ä¸­æœ‰è§£å†³æ–¹æ¡ˆï¼Œæœ€ç»ˆæ˜¯æ›´æ–°äº† numpy ç‰ˆæœ¬è§£å†³çš„ã€‚
